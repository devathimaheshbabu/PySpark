{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276f2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7d56f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32622cd",
   "metadata": {},
   "source": [
    "## Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6861a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d348bd",
   "metadata": {},
   "source": [
    "#### Reading a csv file with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c20032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(r\"original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252823ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f8544",
   "metadata": {},
   "source": [
    "There are null present in some columns of the dataframe before moving to the next analysis need to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e355fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Created a new column named clean_city in which replacing null values which are present in city column with \"UnKnown\"\n",
    "df1 = df.withColumn(\"Clean_City\",when(df.City.isNull(), 'UnKnown').otherwise(df.City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering the data frame with the records where there are no null values present in the Jobtitle\n",
    "df1 = df1.filter(df1.JobTitle.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dccf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517757c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing the null value of salary column with mean of salary column\n",
    "df1 = df1.withColumn('Clean_Salary',df1.Salary.substr(2,100).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e30934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2cd21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df1.groupBy().avg('Clean_Salary').take(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeed2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('new_salary', when(df1.Clean_Salary.isNull(), lit(mean)).otherwise(df1.Clean_Salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## populate the median value of latitude and longitude\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a249fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = df1.select(\"Latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = lat.filter(lat.Latitude.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6476ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = lat.withColumn('latitude2',lat.Latitude.cast('float')).select('latitude2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875181ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "medain = np.median(lat.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "medain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('lat_new', when(df1.Latitude.isNull(), lit(medain)).otherwise(df1.Latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 - Overall the mean or women get paid more on average?\n",
    "#Q2 - By Job Title do men or women get paid more on average?\n",
    "#Q3 - By Which City has the highest average Salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd7000",
   "metadata": {},
   "source": [
    "#### Answering Q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sqlfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = df1.groupBy('gender').agg(sqlfunc.avg('new_salary').alias('AvgSalary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39acdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3bfda",
   "metadata": {},
   "source": [
    "### Answering Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('female_salary', when(df1.gender == 'Female', df1.new_salary).otherwise(lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df10edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('male_salary', when(df2.gender == 'Male', df2.new_salary).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fdfdd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.groupBy(\"JobTitle\").agg(sqlfunc.avg('female_salary').alias('final_female_salary'), sqlfunc.avg('male_salary').alias('final_male_salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e72d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('Delta', df2.final_female_salary - df2.final_male_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2106481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a504468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfb2b15b",
   "metadata": {},
   "source": [
    "### Answering Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityavg = df1.groupBy('City').agg(sqlfunc.avg('new_salary').alias('avgsalary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityavg = cityavg.sort(col('avgsalary').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityavg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3dc3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef340d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fba8e5f4",
   "metadata": {},
   "source": [
    "### Bringing Data into Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the csv in another way in spark\n",
    "df_n = spark.read.csv(r'original.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30003184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19452dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of the attributes present in the dataframe\n",
    "df_n.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38b995",
   "metadata": {},
   "source": [
    "As the spark interpretted every column as string eventhough there are some interger or float attribute present need to change them from string to thier respective data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the data types of the attribues.\n",
    "from pyspark.sql.types import *\n",
    "# As one can define their schema in sql here also we can  define the schema prior itself of the data which we will be \n",
    "# loading in the spark session. So the schema goes likes this\n",
    "\n",
    "schema = StructType([StructField('id', IntegerType()),\n",
    "                     StructField('first_name', StringType()),\n",
    "                     StructField('last_name', StringType()),\n",
    "                     StructField('gender',StringType()),\n",
    "                     StructField('City', StringType()),\n",
    "                     StructField('JobTitle', StringType()),\n",
    "                     StructField('Salary', StringType()),\n",
    "                     StructField('Latitude', StringType()),\n",
    "                     StructField('Longitude', FloatType())])\n",
    "\n",
    "df4 = spark.read.csv(r'original.csv', header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b493f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_n = df4.withColumn('Latitude', df4.Longitude.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_n.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3314d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ae75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13387ae6",
   "metadata": {},
   "source": [
    "### Inspecting a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a323f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('id', IntegerType()),\n",
    "                     StructField('first_name', StringType()),\n",
    "                     StructField('last_name', StringType()),\n",
    "                     StructField('gender',StringType()),\n",
    "                     StructField('City', StringType()),\n",
    "                     StructField('JobTitle', StringType()),\n",
    "                     StructField('Salary', StringType()),\n",
    "                     StructField('Latitude', FloatType()),\n",
    "                     StructField('Longitude', FloatType())])\n",
    "\n",
    "df_nn = spark.read.csv(r'original.csv', header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e68a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d419317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39114053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd81ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f71e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f87ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a645607",
   "metadata": {},
   "source": [
    "### Handling Nulls and Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda704c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping all the records from the data where ever nulls are present in any of the column\n",
    "df_dropped = df_nn.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aada4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the records from the dataset where nulls are present in the jobtitle column\n",
    "df_not_null_jobs = df_nn.filter(df_nn.JobTitle.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_null_jobs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e69de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating a new column if there is a null present in a column replace null with unknown or keep what is was there.\n",
    "df_handled = df_nn.withColumn(\"Clean_City\", when(df_nn.City.isNull(),\"Unknown\").otherwise(df_nn.City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd014854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the duplicates from the dataset\n",
    "df_no_duplicates = df_nn.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22aa519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3355a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "527f76cd",
   "metadata": {},
   "source": [
    "### Selecting and filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select particular columns from the dataframe\n",
    "df_nn.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c385d",
   "metadata": {},
   "source": [
    "Let's Select first_name and Last_name from the above dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40764b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_nn.select(\"first_name\",\"last_name\")\n",
    "df_select.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Renaming a column\n",
    "df_rename_firstname = df_nn.withColumnRenamed('first_name','fn')\n",
    "df_rename_firstname.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering the dataframe for a particular attribute from a particular column\n",
    "df_filter = df_nn.filter(df_nn.first_name == 'Alvera')\n",
    "df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering the dataframe with a wildcard expression from a particular column\n",
    "df_filter_we = df_nn.filter(df_nn.first_name.like('%lver%'))\n",
    "df_filter_we.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering a dataframe based on  a particular column where the values endwith a particular pattern\n",
    "df_filter_ew = df_nn.filter(df_nn.first_name.endswith('din'))\n",
    "df_filter_ew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering a dataframe based on  a particular column where the values endwith a particular pattern\n",
    "df_filter_sw = df_nn.filter(df_nn.first_name.startswith('Alv'))\n",
    "df_filter_sw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filtering a numerical column between certain range of values\n",
    "df_filter_btw = df_nn.filter(df_nn.id.between(1,5))\n",
    "df_filter_btw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filtering the dataframe based on more than one attribute from a particular column.\n",
    "df_filter_fnm = df_nn.filter((df_nn.first_name.isin('Aldin','Valma')))\n",
    "df_filter_fnm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2bb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## breaking the values of a particular column to certain characters and creating a new column with it.(Substring)\n",
    "df_subs = df_nn.select(df_nn.first_name, df_nn.first_name.substr(1,5).alias('New_Name'))\n",
    "df_subs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30224c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fae333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f28e70e5",
   "metadata": {},
   "source": [
    "### Applying multiple Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scenario - 1\n",
    "df_filt1 = df_nn.filter((df_nn.first_name.isin('Aldin','Valma')) | (df_nn.City.like(\"%ondon\")))\n",
    "df_filt1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797b016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scenario - 2\n",
    "df_filt2 = df_nn.filter((df_nn.id > 10) & (df_nn.id < 100))\n",
    "df_filt2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51ac34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d9f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c69454",
   "metadata": {},
   "source": [
    "### Running SQL on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e745488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.registerTempTable('original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = spark.sql('select * from original')\n",
    "query1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a92408",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = spark.sql('select concat(first_name,\" \",last_name) as full_name from original')\n",
    "query2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e6f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f74f0a",
   "metadata": {},
   "source": [
    "### Adding Calculated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc = df_nn.withColumn('Clean_Salary', df_nn.Salary.substr(2,100).cast('float'))\n",
    "df_cc.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mn_slr = df_cc.withColumn('Monthly_Salary', df_cc.Clean_Salary/12)\n",
    "df_mn_slr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab592ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding a new column to identify whether the customer is a female or not\n",
    "df_fc = df_nn.withColumn(\"Are_they_Female\", when(df_nn.gender == 'Female', 'Yes').otherwise('No'))\n",
    "df_fc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f3b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bd1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa240132",
   "metadata": {},
   "source": [
    "### Groupby and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f05bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Groupby gender and get the sum of total salary\n",
    "df_g_s = df_cc.groupBy('gender').agg(sqlfunc.sum('Clean_Salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd347ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g_s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551519dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Groupby gender and get the sum, avg, min and max of clean_salary\n",
    "df_gp_cs = df_cc.groupBy('gender').agg(sqlfunc.sum('Clean_Salary').alias(\"Total\"),\n",
    "                                       sqlfunc.avg('Clean_Salary').alias(\"Average\"),\n",
    "                                       sqlfunc.max('Clean_Salary').alias(\"Maximum\"),\n",
    "                                       sqlfunc.min('Clean_Salary').alias(\"Minimum\"))\n",
    "\n",
    "df_gp_cs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75963a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Groupby gender, city and get the sum, avg, min and max of clean_salary\n",
    "\n",
    "df_g_c_cs = df_cc.groupBy('gender','city').agg(sqlfunc.sum(\"Clean_Salary\").alias(\"Total\"),\n",
    "                                               sqlfunc.avg(\"Clean_Salary\").alias(\"Average\"),\n",
    "                                               sqlfunc.max(\"Clean_Salary\").alias(\"Maximum\"),\n",
    "                                               sqlfunc.min(\"Clean_Salary\").alias(\"Minimum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf3152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g_c_cs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82633193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrting output to a file\n",
    "\n",
    "df_nn.write.csv('df_nn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b694cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe1c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
